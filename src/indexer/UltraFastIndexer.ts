import { cpus } from 'os';
import { promises as fs } from 'fs';
import { join } from 'path';
import { 
  IndexingOptions, 
  IndexingProgress, 
  DEFAULT_INDEXING_OPTIONS,
  SearchResult,
  CodeChunk
} from '../common/types.js';
import { 
  getAllFiles, 
  shouldProcessFile, 
  ensureDir, 
  getFileInfo, 
  calculateContentHash 
} from '../utils.js';

/**
 * Ultra-Fast Indexer - NO EMBEDDINGS APPROACH
 * Uses lexical search, TF-IDF, and AST parsing for maximum speed
 * Targets sub-60-second indexing like Augment
 */
export class UltraFastIndexer {
  private projectPath: string;
  private indexDir: string;
  private options: IndexingOptions;
  private chunks: Map<string, CodeChunk> = new Map();
  private termFrequency: Map<string, Map<string, number>> = new Map(); // term -> chunkId -> frequency
  private documentFrequency: Map<string, number> = new Map(); // term -> document count
  private chunkTerms: Map<string, Set<string>> = new Map(); // chunkId -> terms
  private isIndexing = false;
  private isInitialized = false;

  constructor(
    projectPath: string, 
    indexDir: string, 
    options: Partial<IndexingOptions> = {}
  ) {
    this.projectPath = projectPath;
    this.indexDir = indexDir;
    this.options = { ...DEFAULT_INDEXING_OPTIONS, ...options };
  }

  /**
   * Initialize - NO MODEL LOADING!
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) return;

    console.log('üöÄ Initializing Ultra-Fast Indexer (NO EMBEDDINGS)...');
    const startTime = Date.now();
    
    await ensureDir(this.indexDir);
    await this.loadExistingIndex();
    
    this.isInitialized = true;
    console.log(`‚úÖ Ultra-Fast indexer ready in ${Date.now() - startTime}ms`);
  }

  /**
   * ULTRA-FAST indexing with lexical approach
   */
  async indexCodebase(onProgress?: (progress: IndexingProgress) => void): Promise<void> {
    if (this.isIndexing) {
      throw new Error('Indexing already in progress');
    }

    this.isIndexing = true;
    const startTime = Date.now();

    try {
      console.log('‚ö° Starting ULTRA-FAST indexing (NO EMBEDDINGS)...');
      
      const allFiles = await getAllFiles(this.projectPath, this.options.supportedExtensions, this.options.ignorePatterns);
      const filesToProcess = allFiles.filter(file => 
        shouldProcessFile(file, this.options.supportedExtensions, this.options.ignorePatterns, this.projectPath)
      );
      
      console.log(`üìÅ Found ${filesToProcess.length} files to process`);

      // MAXIMUM concurrency for speed
      const concurrency = Math.min(64, Math.max(16, cpus().length * 8));
      
      // Process all files in parallel batches
      const allChunks: CodeChunk[] = [];
      let processedCount = 0;

      for (let i = 0; i < filesToProcess.length; i += concurrency) {
        const batch = filesToProcess.slice(i, i + concurrency);
        
        const batchPromises = batch.map(async (filePath) => {
          try {
            const chunks = await this.processFileUltraFast(filePath);
            processedCount++;
            
            if (onProgress) {
              onProgress({
                totalFiles: filesToProcess.length,
                filesProcessed: processedCount,
                chunksProcessed: 0,
                totalChunks: 0,
                currentFile: filePath,
                phase: 'parsing',
                errors: []
              });
            }
            
            return chunks;
          } catch (error) {
            console.warn(`‚ö†Ô∏è Failed to process ${filePath}: ${error}`);
            return [];
          }
        });

        const batchResults = await Promise.all(batchPromises);
        
        // Flatten and collect all chunks
        for (const chunks of batchResults) {
          allChunks.push(...chunks);
        }

        console.log(`üìä Processed ${Math.min(i + concurrency, filesToProcess.length)}/${filesToProcess.length} files`);
      }

      console.log(`üß† Building lexical index for ${allChunks.length} chunks...`);
      if (onProgress) {
        onProgress({
          totalFiles: filesToProcess.length,
          filesProcessed: processedCount,
          chunksProcessed: 0,
          totalChunks: allChunks.length,
          currentFile: '',
          phase: 'embedding', // Keep same phase name for compatibility
          errors: []
        });
      }

      // Build lexical index (MUCH faster than embeddings)
      await this.buildLexicalIndex(allChunks);
      
      // Store chunks
      for (const chunk of allChunks) {
        this.chunks.set(chunk.id, chunk);
      }

      // Save index
      await this.saveIndex();

      console.log(`üéâ ULTRA-FAST indexing completed in ${(Date.now() - startTime) / 1000}s`);
      console.log(`üìà Performance: ${allChunks.length} chunks processed`);

    } finally {
      this.isIndexing = false;
    }
  }

  /**
   * Ultra-fast file processing
   */
  private async processFileUltraFast(filePath: string): Promise<CodeChunk[]> {
    const fileInfo = await getFileInfo(filePath);
    
    if (fileInfo.size > this.options.maxFileSize) {
      return [];
    }

    const content = await fs.readFile(filePath, 'utf-8');
    const lines = content.split('\n');
    
    // Simple chunking - optimized for speed
    const chunks: CodeChunk[] = [];
    const chunkSize = 30; // Smaller chunks for better search granularity

    for (let i = 0; i < lines.length; i += chunkSize) {
      const chunkLines = lines.slice(i, Math.min(i + chunkSize, lines.length));
      const chunkContent = chunkLines.join('\n');
      
      if (chunkContent.trim().length > 10) {
        const chunk: CodeChunk = {
          id: `${filePath}:${i}:${Math.min(i + chunkSize, lines.length)}`,
          filePath,
          content: chunkContent,
          startLine: i + 1,
          endLine: Math.min(i + chunkSize, lines.length),
          chunkIndex: Math.floor(i / chunkSize),
          fileHash: calculateContentHash(chunkContent),
          lastModified: Date.now(),
          language: fileInfo.language,
          symbols: [] // Skip for speed
        };
        chunks.push(chunk);
      }
    }

    return chunks;
  }

  /**
   * Build lexical index using TF-IDF (MUCH faster than embeddings)
   */
  private async buildLexicalIndex(chunks: CodeChunk[]): Promise<void> {
    console.log(`üöÄ Building lexical index for ${chunks.length} chunks...`);
    const startTime = Date.now();

    // Clear existing index
    this.termFrequency.clear();
    this.documentFrequency.clear();
    this.chunkTerms.clear();

    // Process chunks in parallel batches
    const batchSize = 1000;
    for (let i = 0; i < chunks.length; i += batchSize) {
      const batch = chunks.slice(i, i + batchSize);
      
      // Process batch
      for (const chunk of batch) {
        const terms = this.extractTerms(chunk.content);
        this.chunkTerms.set(chunk.id, terms);
        
        // Update term frequency
        for (const term of terms) {
          if (!this.termFrequency.has(term)) {
            this.termFrequency.set(term, new Map());
          }
          const termMap = this.termFrequency.get(term)!;
          termMap.set(chunk.id, (termMap.get(chunk.id) || 0) + 1);
          
          // Update document frequency
          this.documentFrequency.set(term, (this.documentFrequency.get(term) || 0) + 1);
        }
      }

      console.log(`üìä Indexed ${Math.min(i + batchSize, chunks.length)}/${chunks.length} chunks`);
    }

    const duration = (Date.now() - startTime) / 1000;
    console.log(`‚úÖ Lexical index built in ${duration.toFixed(1)}s`);
  }

  /**
   * Extract terms from content for lexical search
   */
  private extractTerms(content: string): Set<string> {
    const terms = new Set<string>();
    
    // Extract words, identifiers, and code tokens
    const tokens = content
      .toLowerCase()
      .replace(/[^\w\s]/g, ' ') // Replace non-word chars with spaces
      .split(/\s+/)
      .filter(token => token.length > 2 && token.length < 50); // Filter reasonable tokens

    for (const token of tokens) {
      terms.add(token);
    }

    return terms;
  }

  /**
   * Ultra-fast search using TF-IDF scoring
   */
  async search(query: string, topK: number = 10): Promise<SearchResult[]> {
    if (!this.isInitialized) {
      await this.initialize();
    }

    console.log(`üîç Ultra-fast search: "${query}" (${this.chunks.size} chunks)`);
    
    if (this.chunks.size === 0) {
      console.log('‚ö†Ô∏è No indexed data found. Please run indexing first.');
      return [];
    }

    const queryTerms = Array.from(this.extractTerms(query));
    const scores = new Map<string, number>();

    // Calculate TF-IDF scores for each chunk
    for (const [chunkId, chunk] of this.chunks) {
      let score = 0;
      
      for (const term of queryTerms) {
        const tf = this.termFrequency.get(term)?.get(chunkId) || 0;
        const df = this.documentFrequency.get(term) || 0;
        
        if (tf > 0 && df > 0) {
          // TF-IDF calculation
          const tfScore = 1 + Math.log(tf);
          const idfScore = Math.log(this.chunks.size / df);
          score += tfScore * idfScore;
        }
      }

      // Boost for exact matches in file path
      if (chunk.filePath.toLowerCase().includes(query.toLowerCase())) {
        score *= 1.5;
      }

      if (score > 0) {
        scores.set(chunkId, score);
      }
    }

    // Convert to results and sort
    const results: SearchResult[] = [];
    for (const [chunkId, score] of scores) {
      const chunk = this.chunks.get(chunkId);
      if (chunk) {
        results.push({
          chunk,
          score,
          relevance: score,
          distance: 1 / (1 + score) // Convert score to distance-like metric
        });
      }
    }

    results.sort((a, b) => b.score - a.score);
    return results.slice(0, topK);
  }

  /**
   * Load existing index from disk
   */
  private async loadExistingIndex(): Promise<void> {
    const chunksPath = join(this.indexDir, 'chunks.json');
    const indexPath = join(this.indexDir, 'lexical_index.json');

    try {
      // Load chunks
      if (await fs.access(chunksPath).then(() => true).catch(() => false)) {
        const chunksData = await fs.readFile(chunksPath, 'utf-8');
        const chunksArray = JSON.parse(chunksData);
        this.chunks.clear();
        for (const chunk of chunksArray) {
          this.chunks.set(chunk.id, chunk);
        }
        console.log(`üì¶ Loaded ${chunksArray.length} existing chunks`);
      }

      // Load lexical index
      if (await fs.access(indexPath).then(() => true).catch(() => false)) {
        const indexData = await fs.readFile(indexPath, 'utf-8');
        const index = JSON.parse(indexData);

        // Restore Maps from JSON
        this.termFrequency = new Map(
          Object.entries(index.termFrequency).map(([term, chunkMap]) =>
            [term, new Map(Object.entries(chunkMap as Record<string, number>))]
          )
        );
        this.documentFrequency = new Map(Object.entries(index.documentFrequency));
        this.chunkTerms = new Map(
          Object.entries(index.chunkTerms).map(([chunkId, terms]) =>
            [chunkId, new Set(terms as string[])]
          )
        );

        console.log(`üß† Loaded lexical index with ${this.termFrequency.size} terms`);
      }
    } catch (error) {
      console.warn(`‚ö†Ô∏è Failed to load existing index: ${error}`);
    }
  }

  /**
   * Save index to disk
   */
  private async saveIndex(): Promise<void> {
    const chunksPath = join(this.indexDir, 'chunks.json');
    const indexPath = join(this.indexDir, 'lexical_index.json');

    try {
      // Save chunks
      const chunksArray = Array.from(this.chunks.values());
      await fs.writeFile(chunksPath, JSON.stringify(chunksArray, null, 2));

      // Save lexical index (convert Maps to objects for JSON)
      const index = {
        termFrequency: Object.fromEntries(
          Array.from(this.termFrequency.entries()).map(([term, chunkMap]) =>
            [term, Object.fromEntries(chunkMap)]
          )
        ),
        documentFrequency: Object.fromEntries(this.documentFrequency),
        chunkTerms: Object.fromEntries(
          Array.from(this.chunkTerms.entries()).map(([chunkId, terms]) =>
            [chunkId, Array.from(terms)]
          )
        )
      };

      await fs.writeFile(indexPath, JSON.stringify(index, null, 2));
      console.log(`üíæ Saved ${chunksArray.length} chunks and lexical index`);
    } catch (error) {
      console.error(`‚ùå Failed to save index: ${error}`);
      throw error;
    }
  }

  /**
   * Get statistics
   */
  getStats() {
    return {
      totalFiles: new Set(Array.from(this.chunks.values()).map(c => c.filePath)).size,
      totalChunks: this.chunks.size,
      totalRelationships: 0,
      languages: this.getLanguageStats(),
      concepts: {},
      chunkLevels: {},
      averageComplexity: 0,
      indexingTime: 0,
      totalTerms: this.termFrequency.size
    };
  }

  private getLanguageStats(): Record<string, number> {
    const stats: Record<string, number> = {};
    for (const chunk of this.chunks.values()) {
      stats[chunk.language] = (stats[chunk.language] || 0) + 1;
    }
    return stats;
  }

  /**
   * Clear all data
   */
  async clear(): Promise<void> {
    this.chunks.clear();
    this.termFrequency.clear();
    this.documentFrequency.clear();
    this.chunkTerms.clear();

    // Clear files
    const chunksPath = join(this.indexDir, 'chunks.json');
    const indexPath = join(this.indexDir, 'lexical_index.json');

    try {
      await fs.unlink(chunksPath).catch(() => {});
      await fs.unlink(indexPath).catch(() => {});
    } catch (error) {
      // Ignore errors
    }

    console.log('üßπ Ultra-Fast indexer cleared');
  }
}
